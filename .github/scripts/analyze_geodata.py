import os
import json
import subprocess
import datetime

WORKSPACE_DIR = "workspace"
OLD_STATS_FILE = "old_data/stats.json"
STATS_FILE = os.path.join(WORKSPACE_DIR, "stats.json")
README_FILE = os.path.join(WORKSPACE_DIR, "README.md")

def run_command(cmd):
    """è¿è¡Œç³»ç»Ÿå‘½ä»¤"""
    try:
        subprocess.check_call(cmd, shell=True, stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        print(f"âš ï¸ Warning: Command failed: {cmd}")

def count_lines(filepath):
    """è®¡ç®—æ–‡ä»¶è¡Œæ•°"""
    try:
        with open(filepath, 'rb') as f:
            return sum(1 for _ in f)
    except:
        return 0

def process_dat_files():
    """éåŽ†ç›®å½•ï¼Œè§£åŒ… dat æ–‡ä»¶ï¼Œå¹¶è¿”å›žç»Ÿè®¡æ•°æ®"""
    current_stats = {}
    
    # éåŽ† workspace ä¸‹çš„æ‰€æœ‰ä½œè€…ç›®å½•
    for author in os.listdir(WORKSPACE_DIR):
        author_path = os.path.join(WORKSPACE_DIR, author)
        if not os.path.isdir(author_path):
            continue
            
        print(f"ðŸ” Analyzing {author}...")
        current_stats[author] = {}

        # éåŽ†ä½œè€…ç›®å½•ä¸‹çš„å­æ–‡ä»¶å¤¹ (geoip, geosite)
        for category in ["geoip", "geosite"]:
            cat_dir = os.path.join(author_path, category)
            if not os.path.exists(cat_dir):
                continue
                
            # æ‰¾åˆ°ç›®å½•ä¸‹çš„ .dat æ–‡ä»¶
            for file in os.listdir(cat_dir):
                if not file.endswith(".dat"):
                    continue
                
                dat_path = os.path.join(cat_dir, file)
                # åˆ›å»ºå¯¼å‡ºç›®å½•ï¼š workspace/Author/geoip/dat_name_export/
                export_dir = os.path.join(cat_dir, f"{file}_text")
                os.makedirs(export_dir, exist_ok=True)
                
                print(f"  -> Extracting {file}...")
                
                # --- GeoIP å¤„ç† ---
                if "geoip" in file.lower():
                    # 1. èŽ·å–åˆ—è¡¨
                    list_file = os.path.join(export_dir, "list.txt")
                    run_command(f"geoip list {dat_path} > {list_file}")
                    
                    # 2. è¯»å–åˆ—è¡¨å¹¶å¯¼å‡ºæ¯ä¸ª tag
                    if os.path.exists(list_file):
                        with open(list_file, 'r') as f:
                            tags = [line.strip().split()[0] for line in f if line.strip()]
                        
                        # ä»…å¯¼å‡ºå¸¸ç”¨ Tag é˜²æ­¢æ–‡ä»¶è¿‡å¤š (å¯é€‰ï¼šå¦‚æžœæƒ³å¯¼å‡ºæ‰€æœ‰ï¼ŒåŽ»æŽ‰ [:20])
                        # ä¸ºäº†æ¼”ç¤ºï¼Œè¿™é‡Œå¯¼å‡ºæ‰€æœ‰ï¼Œä½†å®žé™…ä½¿ç”¨å»ºè®®åšä¸ªè¿‡æ»¤ï¼Œå¦åˆ™å¯èƒ½æœ‰å‡ ç™¾ä¸ªæ–‡ä»¶
                        for tag in tags: 
                            out_txt = os.path.join(export_dir, f"{tag}.txt")
                            run_command(f"geoip export -o {out_txt} {dat_path} {tag}")
                            
                            # ç»Ÿè®¡
                            count = count_lines(out_txt)
                            current_stats[author][f"{file}::{tag}"] = count

                # --- GeoSite å¤„ç† ---
                elif "geosite" in file.lower() or "dlc" in file.lower():
                    # Geosite å·¥å…·é€šå¸¸ç›´æŽ¥æ”¯æŒå¯¼å‡º
                    # å…ˆå°è¯•åˆ—å‡º (domain-list-community æ²¡æœ‰ç®€å•çš„ list å‘½ä»¤ï¼Œé€šå¸¸ç›´æŽ¥è§£åŒ…)
                    # è¿™é‡Œå‡è®¾æˆ‘ä»¬åªå…³å¿ƒå¸¸è§åˆ†ç±»ï¼Œæˆ–è€…å°è¯•å¯¼å‡ºç‰¹å®šåˆ—è¡¨
                    # ä¹Ÿå¯ä»¥ç”¨ tool éåŽ†ï¼Œè¿™é‡Œç®€åŒ–é€»è¾‘ï¼Œå°è¯•å¯¼å‡º Google, CN, Apple ç­‰å¸¸è§
                    
                    # å®žé™…ä¸Š domain-list-community å¯ä»¥é€šè¿‡ export å¯¼å‡ºæ‰€æœ‰åŒ…å«çš„ category
                    # ä½†éœ€è¦çŸ¥é“åå­—ã€‚é€šå¸¸åšæ³•æ˜¯è§£åŒ… data ç›®å½•ã€‚
                    # ç”±äºŽå‘½ä»¤è¡Œå·¥å…·é™åˆ¶ï¼Œè¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿå¯¼å‡ºå‡ ä¸ªå…³é”® tag
                    
                    target_tags = ["google", "cn", "apple", "telegram", "netflix", "openai", "category-ads-all"]
                    
                    for tag in target_tags:
                        out_txt = os.path.join(export_dir, f"{tag}.txt")
                        # geosite (domain-list-community) è¯­æ³•: -dat path -export tag
                        # æ³¨æ„ï¼šä¸åŒç‰ˆæœ¬å·¥å…·å‚æ•°å¯èƒ½ä¸åŒï¼Œè¿™é‡Œä½¿ç”¨ domain-list-community æ ‡å‡†
                        run_command(f"geosite -dat {dat_path} -export {tag} > {out_txt}")
                        
                        if os.path.exists(out_txt) and os.path.getsize(out_txt) > 0:
                            count = count_lines(out_txt)
                            current_stats[author][f"{file}::{tag}"] = count
                        else:
                            # æ¸…ç†ç©ºæ–‡ä»¶
                            if os.path.exists(out_txt): os.remove(out_txt)

    return current_stats

def generate_markdown(current_stats, old_stats):
    """ç”Ÿæˆ README.md"""
    lines = ["# ðŸŒ GeoData Assets & Analytics", ""]
    lines.append(f"> Last Updated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC+8)")
    lines.append("")
    
    lines.append("## ðŸ“Š è§„åˆ™ç»Ÿè®¡ä¸Žå˜åŒ–")
    
    for author, rules in current_stats.items():
        if not rules: continue
        
        lines.append(f"### ðŸ‘¤ {author}")
        lines.append("| æ–‡ä»¶::æ ‡ç­¾ | æ¡ç›®æ•°é‡ | è¾ƒæ˜¨æ—¥å˜åŒ– |")
        lines.append("|---|---|---|")
        
        for key, count in sorted(rules.items()):
            # è®¡ç®— Diff
            old_count = old_stats.get(author, {}).get(key, 0)
            diff = count - old_count
            
            diff_str = "0"
            if diff > 0: diff_str = f"ðŸ”º +{diff}"
            elif diff < 0: diff_str = f"ðŸ”» {diff}"
            
            lines.append(f"| {key} | {count} | {diff_str} |")
        lines.append("")

    lines.append("## ðŸ“‚ ç›®å½•ç»“æž„è¯´æ˜Ž")
    lines.append("- **geoip/**: äºŒè¿›åˆ¶ geoip.dat")
    lines.append("- **geosite/**: äºŒè¿›åˆ¶ geosite.dat")
    lines.append("- **xxx_text/**: è§£åŒ…åŽçš„æ–‡æœ¬è§„åˆ™ (æ–¹ä¾¿ Grep æˆ– è½¬æ¢)")
    
    with open(README_FILE, "w", encoding='utf-8') as f:
        f.write("\n".join(lines))
    
    # ä¿å­˜å½“å‰çš„ stats ä»¥å¤‡ä¸‹æ¬¡å¯¹æ¯”
    with open(STATS_FILE, "w", encoding='utf-8') as f:
        json.dump(current_stats, f, indent=2)

def main():
    print("â³ Loading old stats...")
    old_stats = {}
    if os.path.exists(OLD_STATS_FILE):
        try:
            with open(OLD_STATS_FILE, 'r') as f:
                old_stats = json.load(f)
        except:
            print("Old stats file corrupted, skipping diff.")

    print("â³ Processing assets...")
    current_stats = process_dat_files()
    
    print("â³ Generating report...")
    generate_markdown(current_stats, old_stats)
    print("âœ… Done.")

if __name__ == "__main__":
    main()
